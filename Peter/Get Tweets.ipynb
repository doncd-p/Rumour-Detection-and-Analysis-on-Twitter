{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afecec3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:27.548121Z",
     "start_time": "2022-04-27T13:11:27.534087Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# os.environ['BEARER_TOKEN'] = 'WRITE YOUR TOKEN HERE'\n",
    "bearer_token = os.environ.get('BEARER_TOKEN')\n",
    "\n",
    "def create_url(_ids):\n",
    "    ids = \"ids={}\".format(_ids)\n",
    "    tweet_fields = \"tweet.fields=id,text,author_id,conversation_id,created_at,entities,geo,in_reply_to_user_id,lang,possibly_sensitive,public_metrics\"\n",
    "    expansion = \"expansions=author_id\"\n",
    "    user_fields = \"user.fields=id,name,username,created_at,description,entities,location,url,verified,withheld\"\n",
    "    \n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}&{}&{}\".format(ids, tweet_fields, expansion, user_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_tweets(_ids):\n",
    "    url = create_url(_ids)\n",
    "    payload={}\n",
    "    headers = {\n",
    "      'Authorization': 'Bearer {}'.format(bearer_token),\n",
    "      'Cookie': 'guest_id=v1%3A164999832374703747; guest_id_ads=v1%3A164999832374703747; guest_id_marketing=v1%3A164999832374703747; personalization_id=\"v1_+F68isE/iukb7yr8y66bOw==\"'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21b7489d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:33.435873Z",
     "start_time": "2022-04-27T13:11:31.923835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(file_name):\n",
    "    '''\n",
    "    This function will read the file containing tweet ids sperating by lines (in groups)\n",
    "    '''\n",
    "    all_ids = pd.read_table(file_name,sep='\\n',header=None)\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db10d2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:34.443837Z",
     "start_time": "2022-04-27T13:11:34.432844Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def separate_ids(ids):\n",
    "    '''\n",
    "    This function will split the sperate ids into a list\n",
    "    '''\n",
    "    seperated_ids = ids.split(\",\")\n",
    "    return seperated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85180d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T08:30:59.571933Z",
     "start_time": "2022-04-22T08:30:59.560902Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_tweet_groups(groups_ids):\n",
    "    '''\n",
    "    This function will get tweets information using Twitter API in groups with indexing\n",
    "    params groups_ids: all ids in lines (groups) retrieved from the orginal file \n",
    "    '''\n",
    "    tweet_groups = {}\n",
    "    no_access_first_tweets = []\n",
    "    for group_index in range(len(groups_ids)):\n",
    "        group_separated_ids = separate_ids(groups_ids[0][group_index])\n",
    "        group_dict = {}\n",
    "        \n",
    "        # first check if the first tweet is valid to be accessed\n",
    "        # if no access, only 'errors' will be returned\n",
    "        first_tweet = get_tweets(group_separated_ids[0])\n",
    "        if len(first_tweet) == 1:\n",
    "            no_access_first_tweets.append(group_index)\n",
    "            continue\n",
    "        else:\n",
    "            # check if the group contains more than 100 ids \n",
    "            # since multiple tweets lookup has 100 maxmimum\n",
    "            if len(group_separated_ids) > 100:\n",
    "                index = 0\n",
    "                partial_group = {}\n",
    "\n",
    "                # partition groups with more than 100 ids into maximum 100 each group\n",
    "                for i in range(len(group_separated_ids)//100 + 1):\n",
    "                    partial_group_ids = ','.join(group_separated_ids[index:index+100])\n",
    "                    partial_group = get_tweets(partial_group_ids)\n",
    "                    # initialise the dict with the information from the first partial group\n",
    "                    if len(group_dict) == 0 and len(partial_group) != 1:\n",
    "                        group_dict = partial_group\n",
    "                    # combining partial groups into a whole group\n",
    "                    else:\n",
    "                        # there is any tweet valid to be accessed in the partial group\n",
    "                        if len(partial_group) != 1:\n",
    "                            group_dict['data'].extend(partial_group['data'])\n",
    "                            group_dict['includes']['users'].extend(partial_group['includes']['users'])\n",
    "\n",
    "                    index += 100\n",
    "\n",
    "            else:\n",
    "                full_group_ids = ','.join(group_separated_ids)\n",
    "                group_dict = get_tweets(full_group_ids)\n",
    "\n",
    "            tweet_groups[group_index] = group_dict\n",
    "    return tweet_groups, no_access_first_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0f152b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:36.598838Z",
     "start_time": "2022-04-27T13:11:36.577838Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_tweet_groups_start_end(groups_ids, start, end):\n",
    "    '''\n",
    "    This function will get tweets information using Twitter API in groups with indexing\n",
    "    params groups_ids: all ids in lines (groups) retrieved from the orginal file \n",
    "    '''\n",
    "    tweet_groups = {}\n",
    "    for group_index in range(start, end):\n",
    "        group_separated_ids = separate_ids(groups_ids[0][group_index])\n",
    "        group_dict = {}\n",
    "        \n",
    "        # first check if the first tweet is valid to be accessed\n",
    "        # if no access, only 'errors' will be returned\n",
    "        first_tweet = get_tweets(group_separated_ids[0])\n",
    "        if len(first_tweet) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            # check if the group contains more than 100 ids \n",
    "            # since multiple tweets lookup has 100 maxmimum\n",
    "            if len(group_separated_ids) > 100:\n",
    "                index = 0\n",
    "                partial_group = {}\n",
    "\n",
    "                # partition groups with more than 100 ids into maximum 100 each group\n",
    "                for i in range(len(group_separated_ids)//100 + 1):\n",
    "                    if len(group_separated_ids) % 100 == 0 and i == len(group_separated_ids)//100:\n",
    "                        continue\n",
    "                        \n",
    "                    partial_group_ids = ','.join(group_separated_ids[index:index+100])\n",
    "                    partial_group = get_tweets(partial_group_ids)\n",
    "\n",
    "                    # initialise the dict with the information from the first partial group\n",
    "                    if len(group_dict) == 0 and len(partial_group) != 1:\n",
    "                        group_dict = partial_group\n",
    "                    # combining partial groups into a whole group\n",
    "                    else:\n",
    "                        # there is any tweet valid to be accessed in the partial group\n",
    "                        if len(partial_group) != 1:\n",
    "                            group_dict['data'].extend(partial_group['data'])\n",
    "                            group_dict['includes']['users'].extend(partial_group['includes']['users'])\n",
    "\n",
    "                    index += 100\n",
    "\n",
    "            else:\n",
    "                full_group_ids = ','.join(group_separated_ids)\n",
    "                group_dict = get_tweets(full_group_ids)\n",
    "\n",
    "            tweet_groups[group_index] = group_dict\n",
    "    return tweet_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e52b7c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T13:19:47.617827Z",
     "start_time": "2022-04-21T13:19:47.570826Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc8ca916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:40.567995Z",
     "start_time": "2022-04-27T13:11:40.516970Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids = read_file('../../project-data/train.data.txt')\n",
    "# train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d4176f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:42.164641Z",
     "start_time": "2022-04-27T13:11:42.111717Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_ids = read_file('../../project-data/dev.data.txt')\n",
    "# dev_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fcecca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dbbbcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:11:47.068399Z",
     "start_time": "2022-04-27T13:11:47.054399Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_tweet_data(ids, start, end, file):\n",
    "    new_groups = get_tweet_groups_start_end(ids, start, end)\n",
    "    \n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        tweets = json.load(f)\n",
    "        \n",
    "    tweets.update(new_groups)\n",
    "    \n",
    "    with open(file, \"w\",encoding='utf-8') as outfile:\n",
    "        json.dump(tweets, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d72390",
   "metadata": {},
   "source": [
    "### Devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac73eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tweet_data(dev_ids, 0, 100, \"data/dev_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is correct \n",
    "with open(\"data/dev_data.json\",'r',encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "# DO NOT PRINT IT DIRECTLY!!!\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b11fb218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-21T13:31:14.468491Z",
     "start_time": "2022-04-21T13:29:02.462325Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# tweets_dev1 = get_tweet_groups_start_end(dev_ids, 0, 100)\n",
    "# tweets_dev2 = get_tweet_groups_start_end(dev_ids, 100, 200)\n",
    "# tweets_dev3 = get_tweet_groups_start_end(dev_ids, 200, 300)\n",
    "# tweets_dev4 = get_tweet_groups_start_end(dev_ids, 300, 400)\n",
    "# tweets_dev5 = get_tweet_groups_start_end(dev_ids, 400, 500)\n",
    "# tweets_dev6 = get_tweet_groups_start_end(dev_ids, 500, 632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ea1afe38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-22T19:12:38.227603Z",
     "start_time": "2022-04-22T19:10:05.605386Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2477f5c8",
   "metadata": {},
   "source": [
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f70b45ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-27T13:13:50.088541Z",
     "start_time": "2022-04-27T13:11:49.415318Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8784/334068739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mupdate_tweet_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data/train_data.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8784/2139106919.py\u001b[0m in \u001b[0;36mupdate_tweet_data\u001b[1;34m(ids, start, end, file)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnew_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tweet_groups_start_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_data.json'"
     ]
    }
   ],
   "source": [
    "update_tweet_data(train_ids, 0, 100, \"data/train_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is correct \n",
    "with open(\"data/train_data.json\",'r',encoding='utf-8') as f:\n",
    "    tweets = json.load(f)\n",
    "    \n",
    "# DO NOT PRINT IT DIRECTLY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f21c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7726934b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T05:27:45.963799Z",
     "start_time": "2022-04-17T05:27:45.958790Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tweets_train1 = get_tweet_groups_start_end(train_ids, 0, 100)\n",
    "# tweets_train2 = get_tweet_groups_start_end(train_ids, 100, 200)\n",
    "# tweets_train3 = get_tweet_groups_start_end(train_ids, 200, 300)\n",
    "# tweets_train4 = get_tweet_groups_start_end(train_ids, 300, 400)\n",
    "# tweets_train5 = get_tweet_groups_start_end(train_ids, 400, 500)\n",
    "# tweets_train7 = get_tweet_groups_start_end(train_ids, 600, 700)\n",
    "# tweets_train8 = get_tweet_groups_start_end(train_ids, 700, 800)\n",
    "# tweets_train9 = get_tweet_groups_start_end(train_ids, 800, 900)\n",
    "# tweets_train10 = get_tweet_groups_start_end(train_ids, 900, 1000)\n",
    "# tweets_train11 = get_tweet_groups_start_end(train_ids, 1000, 1100)\n",
    "# tweets_train12 = get_tweet_groups_start_end(train_ids, 1100, 1200)\n",
    "# tweets_train13 = get_tweet_groups_start_end(train_ids, 1200, 1300)\n",
    "# tweets_train14 = get_tweet_groups_start_end(train_ids, 1300, 1400)\n",
    "# tweets_train15 = get_tweet_groups_start_end(train_ids, 1400, 1500)\n",
    "# tweets_train16 = get_tweet_groups_start_end(train_ids, 1500, 1600)\n",
    "# tweets_train17 = get_tweet_groups_start_end(train_ids, 1600, 1700)\n",
    "# tweets_train18 = get_tweet_groups_start_end(train_ids, 1700, 1800)\n",
    "# tweets_train19 = get_tweet_groups_start_end(train_ids, 1800, 1895)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
