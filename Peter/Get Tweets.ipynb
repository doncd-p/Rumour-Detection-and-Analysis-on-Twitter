{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47bb1bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T19:26:25.775019Z",
     "start_time": "2022-04-16T19:26:25.754476Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# os.environ['BEARER_TOKEN'] = 'WRITE YOUR TOKEN HERE'\n",
    "bearer_token = os.environ.get('BEARER_TOKEN')\n",
    "\n",
    "def create_url(_ids):\n",
    "#     tweet_fields = \"tweet.fields=lang,author_id,attachments,context_annotations,conversation_id,created_at,entities,geo,id,public_metrics,withheld\"\n",
    "#     expansion = \"expansions=author_id\"\n",
    "#     author_fields = \"user.fields=id,name,username,created_at,description,entities,location,public_metrics,protected,verified,withheld\"\n",
    "    ids = \"ids={}\".format(_ids)\n",
    "    tweet_fields = \"tweet.fields=lang,created_at,geo,id,public_metrics,withheld\"\n",
    "    expansion = \"expansions=author_id\"\n",
    "    user_fields = \"user.fields=created_at,description,public_metrics,protected,verified,withheld\"\n",
    "    \n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}&{}&{}\".format(ids, tweet_fields, expansion, user_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_tweets(_ids):\n",
    "    url = create_url(_ids)\n",
    "    payload={}\n",
    "    headers = {\n",
    "      'Authorization': 'Bearer {}'.format(bearer_token),\n",
    "      'Cookie': 'guest_id=v1%3A164999832374703747; guest_id_ads=v1%3A164999832374703747; guest_id_marketing=v1%3A164999832374703747; personalization_id=\"v1_+F68isE/iukb7yr8y66bOw==\"'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b7489d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T19:25:28.624889Z",
     "start_time": "2022-04-16T19:25:27.840287Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_file(file_name):\n",
    "    '''\n",
    "    This function will read the file containing tweet ids sperating by lines (in groups)\n",
    "    '''\n",
    "    all_ids = pd.read_table(file_name,sep='\\n',header=None)\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db10d2fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T19:25:31.297734Z",
     "start_time": "2022-04-16T19:25:31.280672Z"
    }
   },
   "outputs": [],
   "source": [
    "def separate_ids(ids):\n",
    "    '''\n",
    "    This function will split the sperate ids into a list\n",
    "    '''\n",
    "    seperated_ids = ids.split(\",\")\n",
    "    return seperated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5e4c11bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:27:53.393287Z",
     "start_time": "2022-04-17T03:27:53.372288Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tweet_groups(groups_ids):\n",
    "    '''\n",
    "    This function will get tweets information using Twitter API in groups with indexing\n",
    "    params groups_ids: all ids in lines (groups) retrieved from the orginal file \n",
    "    '''\n",
    "    tweet_groups = {}\n",
    "    for group_index in range(len(groups_ids)):\n",
    "        print(group_index)\n",
    "        group_separated_ids = separate_ids(groups_ids[0][group_index])\n",
    "        group_dict = {}\n",
    "        \n",
    "        # first check if the first tweet is valid to be accessed\n",
    "        # if no access, only 'errors' will be returned\n",
    "        first_tweet = get_tweets(group_separated_ids[0])\n",
    "        if len(first_tweet) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            # check if the group contains more than 100 ids \n",
    "            # since multiple tweets lookup has 100 maxmimum\n",
    "            if len(group_separated_ids) > 100:\n",
    "                index = 0\n",
    "                partial_group = {}\n",
    "\n",
    "                # partition groups with more than 100 ids into maximum 100 each group\n",
    "                for i in range(len(group_separated_ids)//100 + 1):\n",
    "                    partial_group_ids = ','.join(group_separated_ids[index:index+100])\n",
    "                    partial_group = get_tweets(partial_group_ids)\n",
    "                    # initialise the dict with the information from the first partial group\n",
    "                    if len(group_dict) == 0 and len(partial_group) != 1:\n",
    "                        group_dict = partial_group\n",
    "                    # combining partial groups into a whole group\n",
    "                    else:\n",
    "                        # there is any tweet valid to be accessed in the partial group\n",
    "                        if len(partial_group) != 1:\n",
    "                            group_dict['data'].extend(partial_group['data'])\n",
    "                            group_dict['includes']['users'].extend(partial_group['includes']['users'])\n",
    "\n",
    "                    index += 100\n",
    "\n",
    "            else:\n",
    "                full_group_ids = ','.join(group_separated_ids)\n",
    "                group_dict = get_tweets(full_group_ids)\n",
    "\n",
    "            tweet_groups[group_index] = group_dict\n",
    "    return tweet_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "be14c435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:39:02.069858Z",
     "start_time": "2022-04-17T03:39:00.000521Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'title': 'Too Many Requests', 'detail': 'Too Many Requests', 'type': 'about:blank', 'status': 429}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8464/4047207761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;31m# there is any tweet valid to be accessed in the partial group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_group\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[0mgroup_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                     \u001b[0mgroup_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'includes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'users'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'includes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'users'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# group_separated_ids = separate_ids(train_ids[0][180])\n",
    "# group_dict = {}\n",
    "\n",
    "# # first check if the first tweet is valid to be accessed\n",
    "# # if no access, only 'errors' will be returned\n",
    "# first_tweet = get_tweets(group_separated_ids[0])\n",
    "# if len(first_tweet) == 1:\n",
    "#     print(\"no\")\n",
    "# else:\n",
    "#     # check if the group contains more than 100 ids \n",
    "#     # since multiple tweets lookup has 100 maxmimum\n",
    "#     if len(group_separated_ids) > 100:\n",
    "#         index = 0\n",
    "#         partial_group = {}\n",
    "\n",
    "#         # partition groups with more than 100 ids into maximum 100 each group\n",
    "#         for i in range(len(group_separated_ids)//100 + 1):\n",
    "#             print(partial_group)\n",
    "#             partial_group_ids = ','.join(group_separated_ids[index:index+100])\n",
    "#             partial_group = get_tweets(partial_group_ids)\n",
    "#             # initialise the dict with the information from the first partial group\n",
    "#             if len(group_dict) == 0 and len(partial_group) != 1:\n",
    "#                 group_dict = partial_group\n",
    "#             # combining partial groups into a whole group\n",
    "#             else:\n",
    "#                 # there is any tweet valid to be accessed in the partial group\n",
    "#                 if len(partial_group) != 1:\n",
    "#                     group_dict['data'].extend(partial_group['data'])\n",
    "#                     group_dict['includes']['users'].extend(partial_group['includes']['users'])\n",
    "\n",
    "#             index += 100\n",
    "\n",
    "#     else:\n",
    "#         full_group_ids = ','.join(group_separated_ids)\n",
    "#         group_dict = get_tweets(full_group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd932e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T19:25:35.773914Z",
     "start_time": "2022-04-16T19:25:35.737826Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids = read_file('../../project-data/train.data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bff13801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:38:35.311363Z",
     "start_time": "2022-04-17T03:38:35.301359Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_train = get_tweet_groups(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f6f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f42007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31005a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T02:17:39.934391Z",
     "start_time": "2022-04-17T02:17:39.849394Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 74\n",
      "193\n",
      "line 93\n",
      "171\n",
      "line 106\n",
      "165\n",
      "line 180\n",
      "143\n",
      "line 243\n",
      "101\n",
      "line 267\n",
      "101\n",
      "line 279\n",
      "124\n",
      "line 280\n",
      "111\n",
      "line 310\n",
      "237\n",
      "line 360\n",
      "124\n",
      "line 365\n",
      "183\n",
      "line 373\n",
      "312\n",
      "line 391\n",
      "202\n",
      "line 406\n",
      "200\n",
      "line 425\n",
      "200\n",
      "line 499\n",
      "226\n",
      "line 531\n",
      "198\n",
      "line 601\n",
      "152\n",
      "line 625\n",
      "202\n",
      "line 678\n",
      "256\n",
      "line 683\n",
      "217\n",
      "line 711\n",
      "325\n",
      "line 714\n",
      "177\n",
      "line 744\n",
      "185\n",
      "line 769\n",
      "178\n",
      "line 784\n",
      "107\n",
      "line 1055\n",
      "104\n",
      "line 1079\n",
      "125\n",
      "line 1233\n",
      "200\n",
      "line 1369\n",
      "129\n",
      "line 1373\n",
      "220\n",
      "line 1379\n",
      "156\n",
      "line 1461\n",
      "200\n",
      "line 1466\n",
      "190\n",
      "line 1520\n",
      "159\n",
      "line 1609\n",
      "119\n",
      "line 1611\n",
      "135\n",
      "line 1629\n",
      "156\n",
      "line 1630\n",
      "126\n",
      "line 1708\n",
      "140\n",
      "line 1754\n",
      "110\n",
      "line 1804\n",
      "200\n",
      "line 1849\n",
      "118\n",
      "line 1851\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "for group_index in range(len(train_ids)):\n",
    "    group_ids =  train_ids[0][group_index]\n",
    "    if group_ids.count(\",\") + 1 >= 100:\n",
    "        print(\"line\", group_index)\n",
    "        print(group_ids.count(\",\") + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402d8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a5233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be09d0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-16T19:25:14.032Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_data\", \"w\",encoding='utf-8') as outfile:\n",
    "        json.dump(tweets_train, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6bc22",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-16T19:25:14.036Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/train_data\",'r',encoding='utf-8') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1e440",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-16T19:25:14.040Z"
    }
   },
   "outputs": [],
   "source": [
    "pprint(train_data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
