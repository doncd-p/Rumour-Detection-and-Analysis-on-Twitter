{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc4b138-b32e-4714-b84e-d0a7c40731b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19915\\ai\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from LoadData_nlp_new import *\n",
    "from DataSet_nlp_new import *\n",
    "from torch.utils.data import DataLoader\n",
    "from TwitterModel_nlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c5dd7-04a4-452a-b09b-b9098899ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Done loading BERT model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15820188-7dba-4955-b282-d6f8c7598d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/all_data/train_data/554894001946759168.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_44008/1590317873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mload_twitter_dev_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadDataAndProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_tweets_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtweet_data_dev_txt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdev_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_twitter_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mdev_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_twitter_dev_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_submit\\nlp428\\code\\LoadData_nlp_new.py\u001b[0m in \u001b[0;36mloadData\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mlist_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mlist_sub_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mtext_list_sub\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tweet_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweets_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_sub_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mtweeter_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\new_submit\\nlp428\\code\\LoadData_nlp_new.py\u001b[0m in \u001b[0;36mget_tweet_list\u001b[1;34m(self, tweets_file_path, id_list)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mid_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_file_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mjson_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtext_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/all_data/train_data/554894001946759168.json'"
     ]
    }
   ],
   "source": [
    "train_tweets_file_path = \"../data/all_data/train_data/\"\n",
    "tweet_data_train_txt = \"../data/project-data/train.data.txt\"\n",
    "train_label = \"../data/project-data/train.label.txt\"\n",
    "dev_tweets_file_path = \"../data/all_data/dev_data/\"\n",
    "tweet_data_dev_txt = \"../data/project-data/dev.data.txt\"\n",
    "dev_label =  \"../data/project-data/dev.label.txt\"\n",
    "load_twitter_train_data = LoadDataAndProcessing(train_tweets_file_path,tweet_data_train_txt,train_label)\n",
    "#load_twitter_train_data = LoadDataAndProcessing(train_tweets_file_path,tweet_data_train_txt)\n",
    "load_twitter_dev_data = LoadDataAndProcessing(dev_tweets_file_path,tweet_data_dev_txt,dev_label)\n",
    "\n",
    "train_input = load_twitter_train_data.loadData()\n",
    "dev_input = load_twitter_dev_data.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61907c21-57ec-4648-a5a4-39b81ce19cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199d0ed-0299-4070-9973-de726ea20bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TaskOneDataset(train_input)\n",
    "devset = TaskOneDataset(dev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac9573-e4dc-4981-9a1d-5b2ae0ca86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=1, collate_fn=trainset.create_mini_batch)\n",
    "devloader = DataLoader(devset, batch_size=1, collate_fn=devset.create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123804a-ba46-456b-9c69-dd0eb3620b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c13fc-a282-4bd2-a35c-5df8869fb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b366b-34c1-4b65-b680-78b4baa1ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()  # 推論模式\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            outputs = model(*data[:3])\n",
    "            # 前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a05df-7248-4b33-ac11-a1ec76526a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "    \n",
    "model_params = get_learnable_params(model)\n",
    "optimizer = torch.optim.Adam(model_params, lr=1.0e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9472e-79d4-42ee-8a8e-5492a0efc338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "train_acc = []\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    train_acc.append(acc)\n",
    "\n",
    "    print(f\"batch size:1\")\n",
    "    print(f'[epoch {epoch+1}] loss: {running_loss:3f}, acc: {acc:3f}')\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time:{end-start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a95ce-6234-497c-9819-658374bf50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestAndProcessing():\n",
    "    def __init__(self,tweets_file_path,tweet_data):\n",
    "        self.tweets_file_path = tweets_file_path\n",
    "        self.tweet_data = tweet_data\n",
    "\n",
    "    def textProcess(self,text):\n",
    "        text = re.sub(\"@[\\w]*\", \"\", text)  # 去@\n",
    "        text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', \"\", text)  # 去URL\n",
    "        text = re.sub('[\\n\\t-]', '', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    def get_tweet_list(self,tweets_file_path,id_list):\n",
    "        text_list = []\n",
    "        for i in id_list:\n",
    "            file_name = tweets_file_path + i + \".json\"\n",
    "            with open(file_name) as f:\n",
    "                json_data = json.load(f)\n",
    "            text_list.append(json_data[\"text\"])\n",
    "        return text_list\n",
    "    \n",
    "    def loadData(self):\n",
    "        all_ids = pd.read_table(tweet_data,sep='\\n',header=None)\n",
    "        data = []\n",
    "        for i in range(len(all_ids)):\n",
    "            list_id = list(all_ids.iloc[i])\n",
    "            list_sub_id = list_id[0].split(\",\")\n",
    "            text_list_sub =  get_tweet_list(list_sub_id)\n",
    "            data.append(text_list_sub)\n",
    "        tweeter_source = []\n",
    "        tweeter_replay = []\n",
    "        tweeter_id = []\n",
    "        text_p1 = \"\"\n",
    "        text_p2 = \"\"\n",
    "        no_source = 0\n",
    "        for tweet_data_list in data:\n",
    "            temp_source = \"\"\n",
    "            temp_reply = []\n",
    "            source_tweet = tweet_data_list[0]['text']\n",
    "            source_num = 0\n",
    "            for tweet in tweet_data_list:\n",
    "                if  'in_reply_to_user_id' not  in tweet.keys():\n",
    "                    source_num = source_num + 1\n",
    "            if  source_num == 0:\n",
    "                text_p1 = self.textProcess(source_tweet['text']).lower()\n",
    "                temp_source = temp_source + source_tweet\n",
    "                tweeter_id.append(source_tweet['id_str'])\n",
    "            elif  source_num > 1:\n",
    "                for tweet in tweet_data_list:\n",
    "                    if  'in_reply_to_user_id' not  in tweet.keys():\n",
    "                        text_p1 = self.textProcess(tweet['text']).lower()\n",
    "                        temp_source = temp_source + text_p1\n",
    "                        tweeter_id.append(tweeter['id_str'])\n",
    "                    else:\n",
    "                        text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                        res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                        if res_sim >= 0.93:\n",
    "                            pass\n",
    "                        else:\n",
    "                            temp_reply.append(text_p2)\n",
    "            else:\n",
    "                for tweet in tweet_data_list:\n",
    "                    if  'in_reply_to_user_id' not  in tweet.keys():\n",
    "                        text_p1 = self.textProcess(tweet['text']).lower()\n",
    "                        temp_source = temp_source + text_p1\n",
    "                else:\n",
    "                    text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                    res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                    if res_sim >= 0.93:\n",
    "                        pass\n",
    "                    elif res_sim <= 0.01:\n",
    "                        pass\n",
    "                    else:\n",
    "                        temp_reply.append(text_p2)\n",
    "            no_source = no_source +1    \n",
    "            tweeter_source.append(temp_source)                          \n",
    "            tweeter_replay.append(temp_reply)     \n",
    "\n",
    "        total_replay = []\n",
    "        for i in tweeter_replay:\n",
    "            temp = ''\n",
    "            for line in i:\n",
    "                temp += line\n",
    "            total_replay.append(temp)\n",
    "\n",
    "        input_value = []\n",
    "        for i in range(len(label)):\n",
    "            temp = {}\n",
    "            temp['text'] = tweeter_source[i]\n",
    "            temp['textb'] = total_replay[i]\n",
    "            input_value.append(temp)\n",
    "\n",
    "        return input_value,tweeter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c92228-edb0-4f58-8915-7feff2a4676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestAndProcessing():\n",
    "    def __init__(self,tweets_file_path,tweet_data):\n",
    "        self.tweets_file_path = tweets_file_path\n",
    "        self.tweet_data = tweet_data\n",
    "\n",
    "    def textProcess(self,text):\n",
    "        text = re.sub(\"@[\\w]*\", \"\", text)  # 去@\n",
    "        text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', \"\", text)  # 去URL\n",
    "        text = re.sub('[\\n\\t-]', '', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    def get_tweet_list(self,tweets_file_path,id_list):\n",
    "        text_list = []\n",
    "        for i in id_list:\n",
    "            file_name = tweets_file_path + i + \".json\"\n",
    "            with open(file_name) as f:\n",
    "                json_data = json.load(f)\n",
    "            text_list.append(json_data)\n",
    "        return text_list\n",
    "    \n",
    "    def loadData(self):\n",
    "        all_ids = pd.read_table(tweet_data,header=None)\n",
    "        data = []\n",
    "        for i in range(len(all_ids)):\n",
    "            list_id = list(all_ids.iloc[i])\n",
    "            list_sub_id = list_id[0].split(\",\")\n",
    "            text_list_sub =  self.get_tweet_list(tweets_file_path,list_sub_id)\n",
    "            data.append(text_list_sub)\n",
    "        #print(data[0])\n",
    "        tweeter_source = []\n",
    "        tweeter_replay = []\n",
    "        tweeter_id = []\n",
    "        text_p1 = \"\"\n",
    "        text_p2 = \"\"\n",
    "        no_source = 0\n",
    "        for tweet_data_list in data:\n",
    "            temp_source = \"\"\n",
    "            temp_reply = []\n",
    "            #print(tweet_data_list[0])\n",
    "            source_tweet = tweet_data_list[0]\n",
    "            source_num = 0\n",
    "            for tweet in tweet_data_list:\n",
    "                if  not tweet['in_reply_to_status_id']:\n",
    "                    source_num = source_num + 1\n",
    "            if  source_num == 0:\n",
    "                text_p1 = self.textProcess(source_tweet['text']).lower()\n",
    "                temp_source = temp_source + text_p1\n",
    "                tweeter_id.append(source_tweet['id_str'])\n",
    "            elif  source_num > 1:\n",
    "                for tweet in tweet_data_list:\n",
    "                    if  not tweet['in_reply_to_status_id']:\n",
    "                        text_p1 = self.textProcess(tweet['text']).lower()\n",
    "                        temp_source = temp_source + text_p1\n",
    "                        tweeter_id.append(tweet['id_str'])\n",
    "                    else:\n",
    "                        text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                        res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                        if res_sim >= 0.93:\n",
    "                            pass\n",
    "                        else:\n",
    "                            temp_reply.append(text_p2)\n",
    "            else:\n",
    "                for tweet in tweet_data_list:\n",
    "                    if  not tweet['in_reply_to_status_id']:\n",
    "                        text_p1 = self.textProcess(tweet['text']).lower()\n",
    "                        temp_source = temp_source + text_p1\n",
    "                    else:\n",
    "                        text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                        res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                        if res_sim >= 0.93:\n",
    "                            pass\n",
    "                        elif res_sim <= 0.01:\n",
    "                            pass\n",
    "                        else:\n",
    "                            temp_reply.append(text_p2)\n",
    "            no_source = no_source +1    \n",
    "            tweeter_source.append(temp_source)                          \n",
    "            tweeter_replay.append(temp_reply)     \n",
    "\n",
    "        total_replay = []\n",
    "        for i in tweeter_replay:\n",
    "            temp = ''\n",
    "            for line in i:\n",
    "                temp += line\n",
    "            total_replay.append(temp)\n",
    "\n",
    "        input_value = []\n",
    "        for i in range(len(tweeter_source)):\n",
    "            temp = {}\n",
    "            temp['text'] = tweeter_source[i]\n",
    "            temp['textb'] = total_replay[i]\n",
    "            input_value.append(temp)\n",
    "\n",
    "        return input_value,tweeter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfd398-5b21-4691-a26c-13994a7dde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "tweets_file_path = \"../data/project-data/tweet-objects/tweet-objects/\"\n",
    "tweet_data = \"../data/project-data/test.data.txt\"\n",
    "load_twitter_test_data= LoadTestAndProcessing(tweets_file_path,tweet_data)\n",
    "\n",
    "test_input,twitter_id  = load_twitter_test_data.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d29dc-db93-481c-9633-44ed51160a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = TaskTestDataset(test_input)\n",
    "testloader = DataLoader(testset, batch_size=1, collate_fn=testset.create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394de58a-db59-4968-b19d-4a83bd0bc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,dataloader):\n",
    "    result_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors, \n",
    "                            labels=labels)\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8c02d-743d-44aa-aa09-b897d4235db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()  # 推論模式\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            outputs = model(*data[:3])\n",
    "            # 前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "pred = get_predictions(model, testloader, compute_acc=False)\n",
    "#print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdb87e-d68d-4c5a-beca-5f206795e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f9bee-2828-4d71-9f9b-6d8c6432cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for i in pred_list:\n",
    "    if i == 0:\n",
    "        new_list.append(1)\n",
    "    else:\n",
    "        new_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aa0c0-184c-409f-87ac-cec0e625f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test=pd.DataFrame(data=new_list)\n",
    "print(test)\n",
    "import csv\n",
    "n = len(test)\n",
    "nlist = range(0,n)\n",
    "test['Id'] = nlist\n",
    "test.columns = ['Predicted','Id']\n",
    "test[['Id','Predicted']] = test[['Predicted','Id']]\n",
    "test.columns = ['Id','Predicted']\n",
    "test.to_csv('../data/test_nlp_v2.csv',encoding='gbk',index = False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9f6f4-b55d-413e-859a-d8f453690618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934c957-cd8f-4ccd-9268-72259ae60c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
