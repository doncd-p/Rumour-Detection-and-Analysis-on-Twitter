{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e115b04-8ade-4699-a558-74a62bd60027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19915\\ai\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from LoadData_nlp import *\n",
    "from DataSet_nlp import *\n",
    "from torch.utils.data import DataLoader\n",
    "from TwitterModel_nlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8c1087-0871-4124-9837-55f4f3b81245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../data/train_data.json\"\n",
    "train_label_file = \"../data/project-data/train.label.txt\"\n",
    "dev_file = \"../data/clean_dev_data.json\"\n",
    "dev_label_file =  \"../data/project-data/dev.label.txt\"\n",
    "load_twitter_train_data = LoadDataAndProcessing(train_file,train_label_file)\n",
    "load_twitter_dev_data = LoadDataAndProcessing(dev_file,dev_label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe44207-db9c-49fb-bb19-c86ff906d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = load_twitter_train_data.loadData()\n",
    "dev_input = load_twitter_dev_data.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6b95c2-c1b9-4340-9ed8-4d21accb8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dev_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e704c9-aa8e-4bf8-9340-6c660081fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TaskOneDataset(train_input)\n",
    "devset = TaskOneDataset(dev_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216adfc-6a77-449b-a90d-72692f5b90f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07221fdb-7525-4685-a456-de7f0bddc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=2, collate_fn=trainset.create_mini_batch)\n",
    "devloader = DataLoader(devset, batch_size=2, collate_fn=devset.create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2f1552-2925-41fd-9574-b041b6fbe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in trainloader:\n",
    " #   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9577beac-dc59-4462-b4a2-1fc067780062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4f39de-55e4-4930-8636-d31befcaf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53de3200-f25d-4c00-a4ec-eade5cd96669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "    \n",
    "model_params = get_learnable_params(model)\n",
    "optimizer = torch.optim.Adam(model_params, lr=1.0e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7c40d4-9971-4737-97ff-b009245a9224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 0.6333973128598849\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()  # 推論模式\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            outputs = model(*data[:3])\n",
    "            # 前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            #print(pred)\n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                #total += 2\n",
    "                #print((pred == labels.squeeze()).sum().item())\n",
    "                #print(labels)\n",
    "                correct += (pred == labels.squeeze()).sum().item()\n",
    "                #print(labels.size(0),(pred == labels).sum().item())\n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b6dfc8a-0c14-4ac6-adca-a5ac501c2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_from_logits(logits):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    return soft_probs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50bc2fa2-142c-4074-8a84-d4075dd38a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:1\n",
      "[training set : epoch 1] loss: 148.54447945812717, acc: 0.8886756238003839\n",
      "[dev set      : epoch 1] loss: 148.54447945812717, acc: 0.8475247524752475\n",
      "batch size:1\n",
      "[training set : epoch 2] loss: 35.758926741895266, acc: 0.9225847728726807\n",
      "[dev set      : epoch 2] loss: 35.758926741895266, acc: 0.8772277227722772\n",
      "batch size:1\n",
      "[training set : epoch 3] loss: 5.705868512595771, acc: 0.9315419065898912\n",
      "[dev set      : epoch 3] loss: 5.705868512595771, acc: 0.8752475247524752\n",
      "batch size:1\n",
      "[training set : epoch 4] loss: 1.4932080246362602, acc: 0.9155470249520153\n",
      "[dev set      : epoch 4] loss: 1.4932080246362602, acc: 0.8693069306930693\n",
      "batch size:1\n",
      "[training set : epoch 5] loss: 0.444606369943358, acc: 0.9360204734484965\n",
      "[dev set      : epoch 5] loss: 0.444606369943358, acc: 0.8712871287128713\n",
      "batch size:1\n",
      "[training set : epoch 6] loss: 0.2393336309614824, acc: 0.9366602687140115\n",
      "[dev set      : epoch 6] loss: 0.2393336309614824, acc: 0.8732673267326733\n",
      "batch size:1\n",
      "[training set : epoch 7] loss: 0.1433477331265749, acc: 0.9373000639795266\n",
      "[dev set      : epoch 7] loss: 0.1433477331265749, acc: 0.8732673267326733\n",
      "batch size:1\n",
      "[training set : epoch 8] loss: 0.09007558890152723, acc: 0.9392194497760716\n",
      "[dev set      : epoch 8] loss: 0.09007558890152723, acc: 0.8772277227722772\n",
      "batch size:1\n",
      "[training set : epoch 9] loss: 0.058132892450885265, acc: 0.9430582213691618\n",
      "[dev set      : epoch 9] loss: 0.058132892450885265, acc: 0.8792079207920792\n",
      "batch size:1\n",
      "[training set : epoch 10] loss: 0.038141218825330725, acc: 0.9430582213691618\n",
      "[dev set      : epoch 10] loss: 0.038141218825330725, acc: 0.8792079207920792\n",
      "time:5594.03\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "train_acc_list = []\n",
    "dev_acc_list = []\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    _, dev_acc = get_predictions(model, devloader, compute_acc=True)\n",
    "    \n",
    "    train_acc_list.append(train_acc)\n",
    "    dev_acc_list.append(dev_acc)\n",
    "    print(f\"batch size:1\")\n",
    "    print(f'[training set : epoch {epoch+1}] loss: {running_loss}, acc: {train_acc}')\n",
    "    print(f'[dev set      : epoch {epoch+1}] loss: {running_loss}, acc: {dev_acc}')\n",
    "end = time.time()\n",
    "print(f\"time:{end-start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e81f1-814b-4add-a2b2-f599e9a916d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions, all_labels = [], []\n",
    "    model.eval()  # 推論模式\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            outputs = model(*data[:3])\n",
    "            # 前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            #print(logits.data.shape)\n",
    "            if compute_acc:\n",
    "                    predictions = pred.detach().cpu().numpy().reshape(-1).tolist()\n",
    "                    \n",
    "                    labels = data[3].cpu().numpy().reshape(-1).tolist()\n",
    "                    #print(len(labels),len(predictions))\n",
    "                    all_predictions.extend(predictions)\n",
    "                    all_labels.extend(labels)\n",
    "            else:\n",
    "                predictions = pred.detach().cpu().numpy().reshape(-1).tolist()\n",
    "                all_predictions.extend(predictions)\n",
    "\n",
    "        if compute_acc:\n",
    "            auc = accuracy_score(all_labels, all_predictions)\n",
    "            return all_predictions, auc\n",
    "        return all_predictions\n",
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d526d586-d6ba-4677-b834-bd24df6ac8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTestAndProcessing():\n",
    "    def __init__(self,tweets_file_path,tweet_data):\n",
    "        self.tweets_file_path = tweets_file_path\n",
    "        self.tweet_data = tweet_data\n",
    "\n",
    "    def textProcess(self,text):\n",
    "        text = re.sub(\"@[\\w]*\", \"\", text)  # 去@\n",
    "        text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', \"\", text)  # 去URL\n",
    "        text = re.sub('[\\n\\t-]', '', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    def get_tweet_list(self,tweets_file_path,id_list):\n",
    "        text_list = []\n",
    "        for i in id_list:\n",
    "            file_name = tweets_file_path + i + \".json\"\n",
    "            with open(file_name) as f:\n",
    "                json_data = json.load(f)\n",
    "            text_list.append(json_data)\n",
    "        return text_list\n",
    "    \n",
    "    def loadData(self):\n",
    "        all_ids = pd.read_table(tweet_data,header=None)\n",
    "        data = []\n",
    "        for i in range(len(all_ids)):\n",
    "            list_id = list(all_ids.iloc[i])\n",
    "            list_sub_id = list_id[0].split(\",\")\n",
    "            text_list_sub =  self.get_tweet_list(tweets_file_path,list_sub_id)\n",
    "            data.append(text_list_sub)\n",
    "        #print(data[0])\n",
    "        tweeter_source = []\n",
    "        tweeter_replay = []\n",
    "        tweeter_id = []\n",
    "        text_p1 = \"\"\n",
    "        text_p2 = \"\"\n",
    "        no_source = 0\n",
    "        for tweet_data_list in data:\n",
    "            temp_source = \"\"\n",
    "            temp_reply = []\n",
    "            #print(tweet_data_list[0])\n",
    "            source_tweet = tweet_data_list[0]\n",
    "            source_num = 0\n",
    "            for tweet in tweet_data_list:\n",
    "                if  not tweet['in_reply_to_status_id']:\n",
    "                    source_num = source_num + 1\n",
    "            if  source_num == 0:\n",
    "                text_p1 = self.textProcess(source_tweet['text']).lower()\n",
    "                temp_source = temp_source + text_p1\n",
    "                tweeter_id.append(source_tweet['id_str'])\n",
    "                for tweet in tweet_data_list:\n",
    "                        text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                        res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                        temp_reply.append(text_p2)\n",
    "            elif  source_num > 1:\n",
    "                for tweet in tweet_data_list:\n",
    "                    if  not tweet['in_reply_to_status_id']:\n",
    "                        text_p1 = self.textProcess(tweet['text']).lower()\n",
    "                        temp_source = temp_source + text_p1\n",
    "                        tweeter_id.append(tweet['id_str'])\n",
    "                    else:\n",
    "                        text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                        res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                        temp_reply.append(text_p2)\n",
    "            else:\n",
    "                for tweet in tweet_data_list:\n",
    "                    if  not tweet['in_reply_to_status_id']:\n",
    "                        text_p1 = self.textProcess(tweet['text']).lower()\n",
    "                        temp_source = temp_source + text_p1\n",
    "                    else:\n",
    "                        text_p2 = self.textProcess(tweet['text']).lower()\n",
    "                        res_sim = difflib.SequenceMatcher(None, text_p1, text_p2).quick_ratio()\n",
    "                        temp_reply.append(text_p2)\n",
    "            no_source = no_source +1    \n",
    "            tweeter_source.append(temp_source)                          \n",
    "            tweeter_replay.append(temp_reply)     \n",
    "\n",
    "        total_replay = []\n",
    "        for i in tweeter_replay:\n",
    "            temp = ''\n",
    "            for line in i:\n",
    "                temp += line\n",
    "            total_replay.append(temp)\n",
    "\n",
    "        input_value = []\n",
    "        for i in range(len(tweeter_source)):\n",
    "            temp = {}\n",
    "            temp['text'] = tweeter_source[i]\n",
    "            temp['textb'] = total_replay[i]\n",
    "            input_value.append(temp)\n",
    "\n",
    "        return input_value,tweeter_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d2bc393-59b5-4978-bc4a-ed10a86188f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import difflib\n",
    "\n",
    "tweets_file_path = \"../data/project-data/tweet-objects/tweet-objects/\"\n",
    "tweet_data = \"../data/project-data/test.data.txt\"\n",
    "load_twitter_test_data= LoadTestAndProcessing(tweets_file_path,tweet_data)\n",
    "\n",
    "test_input,twitter_id  = load_twitter_test_data.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718b7d6-58a3-4b35-9460-f582e43ed629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "209830f8-3de9-447f-a270-4685c84c1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = TaskTestDataset(test_input)\n",
    "testloader = DataLoader(testset, batch_size=1, collate_fn=testset.create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820d5fc-046e-434e-ad5b-d3164fcca460",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in testset :\n",
    "    print(\"token%%%%%%%%%%%%%%%%%%%%\")\n",
    "    print(i[0])\n",
    "    print(\"se%%%%%%%%%%%%%%%%%%%%\")\n",
    "    print(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27897c88-ceac-4d8a-bdad-491bbbc0e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()  # 推論模式\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            outputs = model(*data[:3])\n",
    "            # 前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "pred = get_predictions(model, testloader, compute_acc=False)\n",
    "#print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b67f0606-7716-4530-a801-f09ec12d6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd1318d-6cfd-4e28-a240-0405e65551ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for i in pred_list:\n",
    "    if i == 0:\n",
    "        new_list.append(1)\n",
    "    else:\n",
    "        new_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a65eb62-fd6b-42b7-a367-f367498876f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "..  ..\n",
      "553  0\n",
      "554  1\n",
      "555  0\n",
      "556  0\n",
      "557  0\n",
      "\n",
      "[558 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test=pd.DataFrame(data=new_list)\n",
    "print(test)\n",
    "import csv\n",
    "n = len(test)\n",
    "nlist = range(0,n)\n",
    "test['Id'] = nlist\n",
    "test.columns = ['Predicted','Id']\n",
    "test[['Id','Predicted']] = test[['Predicted','Id']]\n",
    "test.columns = ['Id','Predicted']\n",
    "test.to_csv('../data/test_nlp_v5.csv',encoding='gbk',index = False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
